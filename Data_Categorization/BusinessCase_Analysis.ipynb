{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>content_en</th>\n",
       "      <th>@</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-06</td>\n",
       "      <td>A doctor who is not very inclusive, it seems t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-12</td>\n",
       "      <td>Although the online schedule showed availabili...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>Didn't answer my wife. After traveling more th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>\\nHe works at the Santo Amaro rehabilitation c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>This psychologist canceled the 1st appointment...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code     type  created_at  \\\n",
       "0           br  opinion  2024-04-06   \n",
       "1           br  opinion  2024-04-12   \n",
       "2           br  opinion  2024-04-22   \n",
       "3           br  opinion  2024-04-27   \n",
       "4           br  opinion  2024-04-29   \n",
       "\n",
       "                                          content_en   @  \n",
       "0  A doctor who is not very inclusive, it seems t... NaN  \n",
       "1  Although the online schedule showed availabili... NaN  \n",
       "2  Didn't answer my wife. After traveling more th... NaN  \n",
       "3  \\nHe works at the Santo Amaro rehabilitation c... NaN  \n",
       "4  This psychologist canceled the 1st appointment... NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Carregar o arquivo CSV\n",
    "df = pd.read_csv('D:/OneDrive - InMotion - Consulting/DocPlanner-BusinessCase/Business_Case_Reference/Case_Data/Business Case PC PM - database - raw_Data.csv', encoding='utf-8')\n",
    "# Exibir as primeiras linhas do dataframe para inspecionar os dados\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores ausentes por coluna:\n",
      "country_code        0\n",
      "type                0\n",
      "created_at          0\n",
      "content_en          2\n",
      "@               44853\n",
      "dtype: int64\n",
      "\n",
      "Percentual de valores ausentes por coluna:\n",
      "country_code      0.000000\n",
      "type              0.000000\n",
      "created_at        0.000000\n",
      "content_en        0.004459\n",
      "@               100.000000\n",
      "dtype: float64\n",
      "\n",
      "Primeiras linhas dos dados após tratamento:\n",
      "  country_code     type  created_at  \\\n",
      "0           br  opinion  2024-04-06   \n",
      "1           br  opinion  2024-04-12   \n",
      "2           br  opinion  2024-04-22   \n",
      "3           br  opinion  2024-04-27   \n",
      "4           br  opinion  2024-04-29   \n",
      "\n",
      "                                          content_en   @  \n",
      "0  A doctor who is not very inclusive, it seems t... NaN  \n",
      "1  Although the online schedule showed availabili... NaN  \n",
      "2  Didn't answer my wife. After traveling more th... NaN  \n",
      "3  \\nHe works at the Santo Amaro rehabilitation c... NaN  \n",
      "4  This psychologist canceled the 1st appointment... NaN  \n"
     ]
    }
   ],
   "source": [
    "# 1. Identificar Colunas com Valores Ausentes\n",
    "# Isso exibirá o número de valores ausentes em cada coluna\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Valores ausentes por coluna:\")\n",
    "print(missing_values)\n",
    "# 2. Analisar a Quantidade de Valores Ausentes\n",
    "# ver como uma porcentagem do total\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nPercentual de valores ausentes por coluna:\")\n",
    "print(missing_percentage)\n",
    "# Exibir as primeiras linhas para confirmar as mudanças\n",
    "print(\"\\nPrimeiras linhas dos dados após tratamento:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas após a remoção:\n",
      "Index(['country_code', 'type', 'created_at', 'content_en'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remover a coluna @ do DataFrame\n",
    "df = df.drop(columns=['@'])\n",
    "# Verificar se a coluna foi removida com sucesso\n",
    "print(\"Colunas após a remoção:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas onde 'content_en' está ausente:\n",
      "      country_code     type  created_at content_en\n",
      "14108           br  opinion  2024-05-29        NaN\n",
      "23885           de  opinion  2024-04-09        NaN\n"
     ]
    }
   ],
   "source": [
    "# Filtrar as linhas onde o valor na coluna 'content_en' está ausente (NaN)\n",
    "missing_content_en = df[df['content_en'].isnull()]\n",
    "# Exibir essas linhas para análise\n",
    "print(\"Linhas onde 'content_en' está ausente:\")\n",
    "print(missing_content_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentual de valores ausentes por coluna:\n",
      "country_code    0.0\n",
      "type            0.0\n",
      "created_at      0.0\n",
      "content_en      0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filtrar as linhas onde o valor na coluna 'content_en' está ausente (NaN)\n",
    "# {{ edit_1 }}\n",
    "df = df[df['content_en'].notnull()]  # Remove linhas com 'content_en' ausente\n",
    "# {{ edit_2 }}\n",
    "# 2. Analisar a Quantidade de Valores Ausentes\n",
    "# ver como uma porcentagem do total\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nPercentual de valores ausentes por coluna:\")\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de linhas no DataFrame: 44,851\n"
     ]
    }
   ],
   "source": [
    "# Exibir o número total de linhas no DataFrame com separador de milhares\n",
    "total_linhas = len(df)\n",
    "print(f\"Número total de linhas no DataFrame: {total_linhas:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "versao 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\blc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\blc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\blc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TextBlob(text)\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Calculate sentiment before preprocessing\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent_en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_sentiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing function to the 'content_en' column of the DataFrame\u001b[39;00m\n\u001b[0;32m     62\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_en\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m, in \u001b[0;36mget_sentiment\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentiment\u001b[39m(text):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Calculate the sentiment score of the input text using TextBlob.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[38;5;241m.\u001b[39mpolarity\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\textblob\\decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\textblob\\blob.py:439\u001b[0m, in \u001b[0;36mBaseBlob.sentiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentiment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a tuple of form (polarity, subjectivity ) where polarity\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m    is a float within the range [-1.0, 1.0] and subjectivity is a float\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)``\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\textblob\\en\\sentiments.py:45\u001b[0m, in \u001b[0;36mPatternAnalyzer.analyze\u001b[1;34m(self, text, keep_assessments)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     Sentiment \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubjectivity\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sentiment(\u001b[38;5;241m*\u001b[39m\u001b[43mpattern_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\textblob\\_text.py:1001\u001b[0m, in \u001b[0;36mSentiment.__call__\u001b[1;34m(self, s, negation, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;66;03m# A string of words.\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# Sentiment(\"a horrible movie\") => (-0.6, 1.0)\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, basestring):\n\u001b[0;32m   1000\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massessments(\n\u001b[1;32m-> 1001\u001b[0m         ((w\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msplit()),\n\u001b[0;32m   1002\u001b[0m         negation,\n\u001b[0;32m   1003\u001b[0m     )\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# A pattern.en.Text.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\textblob\\_text.py:1445\u001b[0m, in \u001b[0;36mParser.find_tokens\u001b[1;34m(self, string, **kwargs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of sentences from the given string.\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m \u001b[38;5;124;03mPunctuation marks are separated from each word by a space.\u001b[39;00m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;66;03m# \"The cat purs.\" => [\"The cat purs .\"]\u001b[39;00m\n\u001b[1;32m-> 1445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpunctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpunctuation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPUNCTUATION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabbreviations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabbreviations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABBREVIATIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinebreak\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m2,}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - InMotion - Consulting\\BusinessCase\\venv\\Lib\\site-packages\\textblob\\_text.py:391\u001b[0m, in \u001b[0;36mfind_tokens\u001b[1;34m(string, punctuation, abbreviations, replace, linebreak)\u001b[0m\n\u001b[0;32m    389\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(t[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    390\u001b[0m         t \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpunctuation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m replace:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# Split trailing punctuation.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mendswith(punctuation):\n\u001b[0;32m    394\u001b[0m         tail\u001b[38;5;241m.\u001b[39mappend(t[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk  # Library for natural language processing (NLP), provides tools for tokenization, lemmatization, and more\n",
    "from nltk.corpus import stopwords  # Module to access lists of stopwords (common words usually removed during text processing)\n",
    "from nltk.tokenize import word_tokenize  # Function to split text into words (tokenization)\n",
    "from nltk.stem import WordNetLemmatizer  # Function to reduce words to their root form (lemmatization)\n",
    "import pandas as pd  # Library for data manipulation and analysis, especially useful for working with tabular data\n",
    "import os  # Library for interacting with the operating system, e.g., for file and directory manipulation\n",
    "import re  # Library for regular expression operations, useful for searching and manipulating patterns in strings\n",
    "import matplotlib.pyplot as plt  # Library for creating graphs and visualizations\n",
    "from wordcloud import WordCloud  # Function to generate word clouds, a visualization that highlights frequent words in a text\n",
    "from textblob import TextBlob  # Library for text processing, provides functionalities like sentiment analysis and grammatical correction\n",
    "\n",
    "# Download necessary NLTK data (tokenizers, stopwords, and WordNet for lemmatization)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Specify the directory for NLTK data storage\n",
    "nltk_data_dir = os.path.join(os.path.expanduser('~'), 'nltk_data')\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    os.makedirs(nltk_data_dir)  # Create the directory if it doesn't exist\n",
    "nltk.data.path.append(nltk_data_dir)  # Add the directory to NLTK's data path\n",
    "\n",
    "# Define a set of words to keep (includes negations and important medical terms)\n",
    "words_to_keep = set(['not', 'no', 'never', 'doctor', 'nurse', 'patient', 'hospital', 'clinic', 'treatment', 'medicine'])\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english')) - words_to_keep  # Remove the words we want to keep from the stopwords list\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by cleaning and normalizing it.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Keep alphanumeric characters, spaces, and important punctuation\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s!?.]', '', text)\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Lemmatize and remove stopwords, but keep important words\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words or word in words_to_keep]\n",
    "        # Join the cleaned tokens back into a single string\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Calculate the sentiment score of the input text using TextBlob.\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Calculate sentiment before preprocessing\n",
    "df['original_sentiment'] = df['content_en'].apply(get_sentiment)\n",
    "\n",
    "# Apply the preprocessing function to the 'content_en' column of the DataFrame\n",
    "df['cleaned_content'] = df['content_en'].apply(preprocess_text)\n",
    "\n",
    "# Calculate sentiment after preprocessing\n",
    "df['cleaned_sentiment'] = df['cleaned_content'].apply(get_sentiment)\n",
    "\n",
    "# Calculate the absolute difference in sentiment scores\n",
    "df['sentiment_difference'] = abs(df['original_sentiment'] - df['cleaned_sentiment'])\n",
    "\n",
    "# Display the original, cleaned content, and sentiment scores\n",
    "print(df[['content_en', 'cleaned_content', 'original_sentiment', 'cleaned_sentiment', 'sentiment_difference']].head(10))\n",
    "\n",
    "# Analyze text length before and after preprocessing\n",
    "df['original_length'] = df['content_en'].str.len()\n",
    "df['cleaned_length'] = df['cleaned_content'].str.len()\n",
    "\n",
    "print(\"\\nAverage text length:\")\n",
    "print(f\"Original: {df['original_length'].mean():.2f}\")\n",
    "print(f\"Cleaned: {df['cleaned_length'].mean():.2f}\")\n",
    "\n",
    "# Specify the output directory for saving files\n",
    "output_dir = \"BusinessCase_Analysis/1_Data_Categorization/1.1_Preprocessing/\"\n",
    "\n",
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['original_length'], bins=50, alpha=0.5, label='Original')\n",
    "plt.hist(df['cleaned_length'], bins=50, alpha=0.5, label='Cleaned')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Length Before and After Preprocessing')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'text_length_distribution.png'))  # Updated path\n",
    "plt.close()\n",
    "print(\"Text length distribution chart saved to:\", os.path.join(output_dir, 'text_length_distribution.png'))\n",
    "\n",
    "# Create word clouds for original and cleaned text\n",
    "def create_wordcloud(text, title):\n",
    "    \"\"\"\n",
    "    Create and save a word cloud visualization for the given text.\n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.savefig(os.path.join(output_dir, f'{title.lower().replace(\" \", \"_\")}.png'))  # Updated path\n",
    "    plt.close()\n",
    "    print(f\"Word cloud '{title}' saved to:\", os.path.join(output_dir, f'{title.lower().replace(\" \", \"_\")}.png'))\n",
    "\n",
    "create_wordcloud(' '.join(df['content_en']), 'Original Text Word Cloud')\n",
    "create_wordcloud(' '.join(df['cleaned_content']), 'Cleaned Text Word Cloud')\n",
    "\n",
    "# Validate preprocessing and sentiment analysis\n",
    "sample_size = min(100, len(df))\n",
    "sample = df.sample(sample_size)\n",
    "\n",
    "def validate_preprocessing(row):\n",
    "    \"\"\"\n",
    "    Validate if preprocessing was effective and sentiment remained consistent.\n",
    "    \"\"\"\n",
    "    original = set(row['content_en'].lower().split())\n",
    "    cleaned = set(row['cleaned_content'].split())\n",
    "    removed = original - cleaned\n",
    "    sentiment_changed = row['sentiment_difference'] > 0.2\n",
    "    return len(removed) > 0 and len(cleaned) > 0 and not sentiment_changed\n",
    "\n",
    "validation_results = sample.apply(validate_preprocessing, axis=1)\n",
    "validation_percentage = (validation_results.sum() / sample_size) * 100\n",
    "\n",
    "print(f\"\\nPreprocessing and Sentiment Validation:\")\n",
    "print(f\"Percentage of samples with effective preprocessing and consistent sentiment: {validation_percentage:.2f}%\")\n",
    "\n",
    "# Calculate the percentage of cases where sentiment changed significantly\n",
    "sentiment_change_threshold = 0.2\n",
    "significant_changes = (df['sentiment_difference'] > sentiment_change_threshold).sum()\n",
    "significant_change_percentage = (significant_changes / len(df)) * 100\n",
    "\n",
    "print(f\"\\nSentiment Change Analysis:\")\n",
    "print(f\"Percentage of samples with significant sentiment change: {significant_change_percentage:.2f}%\")\n",
    "\n",
    "# Sentiment distribution before and after preprocessing\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['original_sentiment'], bins=50, alpha=0.5, label='Original Sentiment')\n",
    "plt.hist(df['cleaned_sentiment'], bins=50, alpha=0.5, label='Cleaned Sentiment')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Scores Before and After Preprocessing')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'sentiment_distribution_comparison.png'))  # Updated path\n",
    "plt.close()\n",
    "print(\"Sentiment distribution chart saved to:\", os.path.join(output_dir, 'sentiment_distribution_comparison.png'))\n",
    "\n",
    "# Document the preprocessing and sentiment analysis steps in the project guide file\n",
    "with open(\"bc_project_guide.md\", \"a\") as file:\n",
    "    file.write(\"\\n## Text Preprocessing and Sentiment Analysis\\n\")\n",
    "    file.write(\"### Text Preprocessing Function\\n\")\n",
    "    file.write(\"We defined an advanced function `preprocess_text` to perform text cleaning while preserving context. This function:\\n\")\n",
    "    file.write(\"- Converts text to lowercase\\n\")\n",
    "    file.write(\"- Removes URLs\\n\")\n",
    "    file.write(\"- Keeps alphanumeric characters, spaces, and important punctuation\\n\")\n",
    "    file.write(\"- Tokenizes the text\\n\")\n",
    "    file.write(\"- Lemmatizes words and removes stopwords, but keeps important medical terms and negations\\n\")\n",
    "    file.write(\"- Joins the cleaned tokens back into a single string\\n\\n\")\n",
    "    file.write(\"We applied this function to the `content_en` column of the DataFrame, creating a new column `cleaned_content` with the cleaned text.\\n\")\n",
    "    file.write(\"\\n### Sentiment Analysis\\n\")\n",
    "    file.write(\"We used TextBlob to perform sentiment analysis on both the original and cleaned text, creating `original_sentiment` and `cleaned_sentiment` columns.\\n\")\n",
    "    file.write(\"\\nWe performed additional analysis:\\n\")\n",
    "    file.write(\"- Compared text lengths before and after preprocessing\\n\")\n",
    "    file.write(\"- Visualized text length distribution (saved as 'text_length_distribution.png')\\n\")\n",
    "    file.write(\"- Created word clouds for original and cleaned text (saved as PNG files)\\n\")\n",
    "    file.write(\"- Validated the preprocessing and sentiment consistency on a sample of the data\\n\")\n",
    "    file.write(f\"- Preprocessing was effective and sentiment remained consistent for {validation_percentage:.2f}% of the sampled data\\n\")\n",
    "    file.write(f\"- {significant_change_percentage:.2f}% of samples showed significant sentiment change after preprocessing\\n\")\n",
    "    file.write(\"- Visualized the distribution of sentiment scores before and after preprocessing (saved as 'sentiment_distribution_comparison.png')\\n\")\n",
    "\n",
    "print(\"Preprocessing, sentiment analysis complete, and documentation updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>content_en</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>original_length</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>original_sentiment</th>\n",
       "      <th>cleaned_sentiment</th>\n",
       "      <th>sentiment_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-06</td>\n",
       "      <td>A doctor who is not very inclusive, it seems t...</td>\n",
       "      <td>doctor not inclusive seems want immediately di...</td>\n",
       "      <td>1018</td>\n",
       "      <td>584</td>\n",
       "      <td>-0.116162</td>\n",
       "      <td>-0.049621</td>\n",
       "      <td>-0.116162</td>\n",
       "      <td>0.066540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-12</td>\n",
       "      <td>Although the online schedule showed availabili...</td>\n",
       "      <td>although online schedule showed availability 1...</td>\n",
       "      <td>154</td>\n",
       "      <td>115</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>Didn't answer my wife. After traveling more th...</td>\n",
       "      <td>didnt answer wife . traveling three hour even ...</td>\n",
       "      <td>246</td>\n",
       "      <td>166</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>0.306667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>\\nHe works at the Santo Amaro rehabilitation c...</td>\n",
       "      <td>work santo amaro rehabilitation center not pre...</td>\n",
       "      <td>624</td>\n",
       "      <td>420</td>\n",
       "      <td>-0.202778</td>\n",
       "      <td>-0.219444</td>\n",
       "      <td>-0.202778</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>br</td>\n",
       "      <td>opinion</td>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>This psychologist canceled the 1st appointment...</td>\n",
       "      <td>psychologist canceled 1st appointment late sec...</td>\n",
       "      <td>636</td>\n",
       "      <td>390</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code     type  created_at  \\\n",
       "0           br  opinion  2024-04-06   \n",
       "1           br  opinion  2024-04-12   \n",
       "2           br  opinion  2024-04-22   \n",
       "3           br  opinion  2024-04-27   \n",
       "4           br  opinion  2024-04-29   \n",
       "\n",
       "                                          content_en  \\\n",
       "0  A doctor who is not very inclusive, it seems t...   \n",
       "1  Although the online schedule showed availabili...   \n",
       "2  Didn't answer my wife. After traveling more th...   \n",
       "3  \\nHe works at the Santo Amaro rehabilitation c...   \n",
       "4  This psychologist canceled the 1st appointment...   \n",
       "\n",
       "                                     cleaned_content  original_length  \\\n",
       "0  doctor not inclusive seems want immediately di...             1018   \n",
       "1  although online schedule showed availability 1...              154   \n",
       "2  didnt answer wife . traveling three hour even ...              246   \n",
       "3  work santo amaro rehabilitation center not pre...              624   \n",
       "4  psychologist canceled 1st appointment late sec...              636   \n",
       "\n",
       "   cleaned_length  sentiment_score  original_sentiment  cleaned_sentiment  \\\n",
       "0             584        -0.116162           -0.049621          -0.116162   \n",
       "1             115         0.100000            0.100000           0.100000   \n",
       "2             166        -0.266667            0.040000          -0.266667   \n",
       "3             420        -0.202778           -0.219444          -0.202778   \n",
       "4             390         0.057143            0.057143           0.057143   \n",
       "\n",
       "   sentiment_difference  \n",
       "0              0.066540  \n",
       "1              0.000000  \n",
       "2              0.306667  \n",
       "3              0.016667  \n",
       "4              0.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "versao 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
